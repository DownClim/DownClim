rule evaluate:
    input:
        proj="results/projection/means/{area}_{project}_{activity}_{domain}_{institute}_{model}_{experiment}_{ensemble}_{rcm}_{downscaling}_{baseline}_{aggregation}_{period_eval}.nc",
        base="results/{base}/means/{area}_{base}_{aggregation}_{period_eval}.nc",
        ds="results/projection/downscaled/{area}_{project}_{activity}_{domain}_{institute}_{model}_{experiment}_{ensemble}_{rcm}_{downscaling}_{baseline}_{aggregation}_{period_eval}_{period_base}_{ds_method}.nc",
        shp="results/countries/{area}.shp"
    output:
        "results/evaluation/{area}_{project}_{activity}_{domain}_{institute}_{model}_{experiment}_{ensemble}_{rcm}_{downscaling}_{baseline}_{aggregation}_{period_eval}_{period_base}_{ds_method}_{base}.nc"
    log:
        "results/logs/evaluate_{area}_{project}_{activity}_{domain}_{institute}_{model}_{experiment}_{ensemble}_{rcm}_{downscaling}_{baseline}_{aggregation}_{period_eval}_{period_base}_{ds_method}_{base}.log"
    benchmark:
        "results/benchmarks/evaluate_{area}_{project}_{activity}_{domain}_{institute}_{model}_{experiment}_{ensemble}_{rcm}_{downscaling}_{baseline}_{aggregation}_{period_eval}_{period_base}_{ds_method}_{base}.benchmark.txt"
    threads: 1
    resources:
        mem_mb=10000
    conda:
        "../envs/xarray.yml"
    params:
        area="{area}",
        project="{project}",
        activity="{activity}",
        domain="{domain}",
        institute="{institute}",
        model="{model}",
        experiment="{experiment}",
        ensemble="{ensemble}",
        rcm="{rcm}",
        baseline="{baseline}",
        period_base="{period_base}",
        period_eval="{period_eval}",
        base_eval="{period_base}"
    script:
      "../scripts/evaluate.py"
