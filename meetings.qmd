# Meetings {.unnumbered}

## 23/11/22 - Kick off

*Ghislain V., Thomas A., Achille M.*

-   Keep it simple first
-   Evaluation: from weather stations, test on an area with a lot of weather stations
-   Evaluation climate data vs baseline climate data
-   Interpolation
    -   bilinear is commonly used
-   Achille previously work downscaling from all available, bioclim computing, and evaluation, for AFRICA and 15 RCM, shell + CDO + wget + GRASS
-   Project differences in moist tropical forest distribution between GCM and RCM
-   Finally provide a tool and a product
-   Have a look to CORDEX downscaling on Europe and North America
-   Maybe add Swiss

## 23/12/05 - Dev summary

Dev points:

-   File naming, for instance longest is results/evaluation/{country}\_{domain}\_{gcm}\_{rcm}\_{rcp}\_{period_eval}\_{period_base}.tsv
-   Modularity:
    -   **Space**: currenlty country level based on GADM, but should be adapted to work at regional scale, global scale, and defined bounding box. Resolution of the baseline.
    -   **Time**: currently resolution of months. I would keep finer resolution, i.e. days or x-hourly, for another version (but maybe more complicated to implement with current structure). I do not think year resolution is useful. Period depend on the different products, for instance beware of the short period of evaluation of CHELSA (2010) which can question the comparison with other such as WorldClim
    -   **Baseline**: currently CHELSA 2.1, add WorldClim? add ERA5-Land?, ready for more?
    -   **Projection**: currently CORDEX, add CMIP5 for comparison, add CMIP6?, add CMIP6 HighRes?
    -   **Scenarios**: all RCPs, add SSPs (depend on CMIP6)?; linked to projection
    -   **Downscaling**: bias correction based on delta / change factor of anomalies only? I would keep more sophisticated quantile based approach for another version (but maybe more complicated to implement with current structure)
    -   **Pre-selection**: currently none. Could be based on climatic range of projections in the defined area. But it is needed for lack of computing power, which I think is not the case here.
    -   **Evaluation**: all available metrics, automatic report. Which data? Currently same as the baseline used. External data could be added for multiple evaluations, e.g. CRU TS 4.
    -   **Post-selection**: currently none. Could be based on previous evaluation. But I do not think it is useful by itself, but could be included eventually in ensemble.
    -   **Ensemble**: currently none. Should be added for general users relying on ensemble projections, while reporting uncertainties (very important). Could be simple multi-model averaging or more complex approaches including Bayesian model averaging.

Python questions/devs:

-   Environment management local / Rstudio / conda with mamba-forge
-   Local / Rstudio envs conflict
-   Very slow conda envs (but working for country)
-   Help on online nc reading from CHELSA or esgf
-   Help on xarray use, but currently pretty autonomous
-   Fresh installation tutorial
-   Updating management of HPC with profiles in snakemake
-   Python/R script for automatic experiment table creation with available CORDEX

## 23/12/05 - Thomas A.
